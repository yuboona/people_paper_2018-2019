{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬取人民日报"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[爬虫教程](https://blog.csdn.net/wenxuhonghe/article/details/90047081) ，按照教程爬取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import os\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### html页面内容爬取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchUrl(url):\n",
    "    '''\n",
    "    功能：访问 url 的网页，获取网页内容并返回\n",
    "    参数：目标网页的 url\n",
    "    返回：目标网页的 html 内容\n",
    "    '''\n",
    "    \n",
    "    headers = {\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',\n",
    "    }\n",
    "    \n",
    "    r = requests.get(url,headers=headers)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取某一天版面链接列表的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageList(year, month, day):\n",
    "    '''\n",
    "    功能：获取当天报纸的各版面的链接列表\n",
    "    参数：年，月，日\n",
    "    '''\n",
    "    url = 'http://paper.people.com.cn/rmrb/html/' + year + '-' + month + '/' + day + '/nbs.D110000renmrb_01.htm'\n",
    "    html = fetchUrl(url)\n",
    "    bsobj = bs4.BeautifulSoup(html,'html.parser')\n",
    "    pageList = bsobj.find('div', attrs = {'id': 'pageList'}).ul.find_all('div', attrs = {'class': 'right_title-name'})\n",
    "    linkList = []\n",
    "    \n",
    "    for page in pageList:\n",
    "        link = page.a[\"href\"]\n",
    "        url = 'http://paper.people.com.cn/rmrb/html/'  + year + '-' + month + '/' + day + '/' + link\n",
    "        linkList.append(url)\n",
    "    \n",
    "    return linkList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取某一版面内的所有文章的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitleList(year, month, day, pageUrl):\n",
    "    '''\n",
    "    功能：获取报纸某一版面的文章链接列表\n",
    "    参数：年，月，日，该版面的链接\n",
    "    '''\n",
    "    html = fetchUrl(pageUrl)\n",
    "    bsobj = bs4.BeautifulSoup(html,'html.parser')\n",
    "    titleList = bsobj.find('div', attrs = {'id': 'titleList'}).ul.find_all('li')\n",
    "    linkList = []\n",
    "    \n",
    "    for title in titleList:\n",
    "        tempList = title.find_all('a')\n",
    "        for temp in tempList:\n",
    "            link = temp[\"href\"]\n",
    "            if 'nw.D110000renmrb' in link:\n",
    "                url = 'http://paper.people.com.cn/rmrb/html/'  + year + '-' + month + '/' + day + '/' + link\n",
    "                linkList.append(url)\n",
    "    \n",
    "    return linkList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析html文本中的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContent(html):\n",
    "    '''\n",
    "    功能：解析 HTML 网页，获取新闻的文章内容\n",
    "    参数：html 网页内容\n",
    "    '''    \n",
    "    bsobj = bs4.BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # 获取文章 标题\n",
    "    title = bsobj.h3.text + '\\n' + bsobj.h1.text + '\\n' + bsobj.h2.text + '\\n'\n",
    "    #print(title)\n",
    "    \n",
    "    # 获取文章 内容\n",
    "    pList = bsobj.find('div', attrs = {'id': 'ozoom'}).find_all('p')\n",
    "    content = ''\n",
    "    for p in pList:\n",
    "        content += p.text + '\\n'      \n",
    "    #print(content)\n",
    "    \n",
    "    # 返回结果 标题+内容\n",
    "    resp = title + content\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存处理好的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFile(content, path, filename):\n",
    "    '''\n",
    "    功能：将文章内容 content 保存到本地文件中\n",
    "    参数：要保存的内容，路径，文件名\n",
    "    '''\n",
    "    # 如果没有该文件夹，则自动生成\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    # 保存文件\n",
    "    with open(path + filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用各组件完成爬取的整个流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_rmrb(year, month, day, destdir):\n",
    "    '''\n",
    "    功能：爬取《人民日报》网站 某年 某月 某日 的新闻内容，并保存在 指定目录下\n",
    "    参数：年，月，日，文件保存的根目录\n",
    "    '''\n",
    "    pageList = getPageList(year, month, day)\n",
    "    for page in pageList:\n",
    "        titleList = getTitleList(year, month, day, page)\n",
    "        for url in titleList:\n",
    "            \n",
    "            # 获取新闻文章内容\n",
    "            html = fetchUrl(url)\n",
    "            content = getContent(html)\n",
    "            \n",
    "            # 生成保存的文件路径及文件名\n",
    "            temp = url.split('_')[2].split('.')[0].split('-')\n",
    "            pageNo = temp[1]\n",
    "            titleNo = temp[0] if int(temp[0]) >= 10 else '0' + temp[0]\n",
    "            path = destdir + '/' + year + month + day + '/'\n",
    "            fileName = year + month + day + '-' + pageNo + '-' + titleNo + '.txt'\n",
    "            \n",
    "            # 保存文件\n",
    "            saveFile(content, path, fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "502 Server Error: Bad Gateway for url: http://paper.people.com.cn/rmrb/html/2019-11/20/nw.D110000renmrb_20191120_11-03.htm",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-85de869e2d62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mday\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mdestdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./data\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mdownload_rmrb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mday\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"爬取完成：\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0myear\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmonth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-1c184f164693>\u001b[0m in \u001b[0;36mdownload_rmrb\u001b[1;34m(year, month, day, destdir)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;31m# 获取新闻文章内容\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetchUrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetContent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-7c3310ee33c1>\u001b[0m in \u001b[0;36mfetchUrl\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapparent_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\miniconda\\envs\\DataProc\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 502 Server Error: Bad Gateway for url: http://paper.people.com.cn/rmrb/html/2019-11/20/nw.D110000renmrb_20191120_11-03.htm"
     ]
    }
   ],
   "source": [
    "# import datetime\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    主函数：程序入口\n",
    "    '''\n",
    "    start_time = datetime.date(2019,6,12)\n",
    "    end_time = datetime.date(2019,11,22)\n",
    "    dates = [(start_time+datetime.timedelta(days=i)) for i in range((end_time-start_time).days+1)]\n",
    "    \n",
    "    for x in dates:\n",
    "        year = str(x.year)\n",
    "        month = str(x.month).rjust(2,'0')\n",
    "        day = str(x.day).rjust(2,'0')\n",
    "        destdir = \"./data\"\n",
    "        download_rmrb(year, month, day, destdir)\n",
    "    print(\"爬取完成：\" + year + month + day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# start_time = datetime.date(2018,1,1)\n",
    "# end_time = datetime.date(2019,11,22)\n",
    "\n",
    "# dates = [(start_time+datetime.timedelta(days=i)) for i in range((end_time-start_time).days+1)]\n",
    "# # print(dates[:10])\n",
    "# for x in dates[:10]:\n",
    "#     print(str(x.year))\n",
    "#     print(str(x.month).rjust(2,'0'))\n",
    "#     print(str(x.day).rjust(2,'0'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataProc",
   "language": "python",
   "name": "dataproc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "80"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
